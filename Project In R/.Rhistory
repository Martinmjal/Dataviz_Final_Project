df_categoricals <- df_categoricals[, !(names(df_categoricals) %in% c("sub_categoria_Detergentes.y.jabones"))]
library(dplyr)
p_values <- numeric(length = ncol(df_categoricals) - 1)
names(p_values) <- setdiff(colnames(df_categoricals), "warehouse_Bogota")
# Realizar las pruebas de chi-cuadrado
for (col in names(p_values)) {
# Crear la tabla de contingencia para cada columna con 'warehouse_Bogota'
contingency_table <- table(df_categoricals$warehouse_Bogota, df_categoricals[[col]])
# Realizar la prueba chi-cuadrado y almacenar el valor p
test_result <- chisq.test(contingency_table)
p_values[col] <- test_result$p.value
}
# Ahora, filtramos los nombres de las columnas con un valor p menor a 0.05
significant_columns <- names(p_values)[p_values > 0.05]
significant_columns <- significant_columns[!is.na(significant_columns)]
significant_columns <- c(significant_columns, "warehouse_Bogota")
significant_columns
df_independent_variables <- subset(df_categoricals, select = significant_columns)
p_values <- numeric(length = ncol(df_independent_variables) - 1)
names(p_values) <- setdiff(colnames(df_independent_variables), "sub_categoria_Ambientadores")
# Realizar las pruebas de chi-cuadrado
for (col in names(p_values)) {
# Crear la tabla de contingencia para cada columna con 'sub_categoria_Ambientadores'
contingency_table <- table(df_independent_variables$sub_categoria_Ambientadores, df_independent_variables[[col]])
# Realizar la prueba chi-cuadrado y almacenar el valor p
test_result <- chisq.test(contingency_table)
p_values[col] <- test_result$p.value
}
# Ahora, filtramos los nombres de las columnas con un valor p menor a 0.05
significant_columns <- names(p_values)[p_values > 0.05]
significant_columns <- significant_columns[!is.na(significant_columns)]
significant_columns <- c(significant_columns, "sub_categoria_Ambientadores")
significant_columns
df_independent_variables <- subset(df_independent_variables, select = significant_columns)
p_values <- numeric(length = ncol(df_independent_variables) - 1)
names(p_values) <- setdiff(colnames(df_independent_variables), "sub_categoria_Complementos.y.vitaminas")
# Realizar las pruebas de chi-cuadrado
for (col in names(p_values)) {
# Crear la tabla de contingencia para cada columna con 'sub_categoria_Complementos.y.vitaminas'
contingency_table <- table(df_independent_variables$sub_categoria_Complementos.y.vitaminas, df_independent_variables[[col]])
# Realizar la prueba chi-cuadrado y almacenar el valor p
test_result <- chisq.test(contingency_table)
p_values[col] <- test_result$p.value
}
# Ahora, filtramos los nombres de las columnas con un valor p menor a 0.05
significant_columns <- names(p_values)[p_values > 0.05]
significant_columns <- significant_columns[!is.na(significant_columns)]
significant_columns <- c(significant_columns, "sub_categoria_Complementos.y.vitaminas")
significant_columns
df_independent_variables <- subset(df_independent_variables, select = significant_columns)
library(reshape2)
cor_matrix <- cor(df[numerical_cols], use = "complete.obs")
# Transforma la matriz de correlación para la visualización
cor_melted <- melt(cor_matrix)
# Configura el tamaño de la fuente para los labels (ajusta según sea necesario)
font_size <- 6
ggplot(data = cor_melted, aes(x = Var1, y = Var2, fill = value)) +
geom_tile() +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1, 1), space = "Lab",
name="Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1, size = font_size),
axis.text.y = element_text(size = font_size)) +
coord_fixed() +
labs(x = '', y = '') +
geom_text(aes(label = sprintf("%.2f", value)), size = 3, check_overlap = TRUE)
columns_for_reduction = c(numerical_cols, significant_columns, 'is_churned')
df_reduction = subset(df, select = columns_for_reduction)
dim(df_reduction)
library(VIM)
aggr_plot <- aggr(df_reduction, col=c('navyblue', 'red'), numbers=FALSE, sortVars=TRUE,
cex.axis=0.4, gap=1, ylab=c("Histogram of missing data", "Pattern"))
porcentaje_nulos <- sapply(df_reduction, function(x) mean(is.na(x))) * 100
df_filtrado <- df_reduction[, porcentaje_nulos < 40]
names(df_filtrado)
library(mice)
porcentaje_nulos <- sapply(df_reduction, function(x) mean(is.na(x))) * 100
df_filtrado <- df_reduction[, porcentaje_nulos < 40]
imp <- mice(df_filtrado[, (names(df_filtrado) %in% c("orders_count_delta_1m", "gmv_delta_1m", "distinct_skus_delta_1m", "orders_count_delta_2m", "gmv_delta_2m", "distinct_skus_delta_2m", "mean_order_lapse", "antiquity_days",
"canceled_orders", "visits_count_3m", "gmv_3m", "AOV_3m", "distinct_sku_3m", "order_count_historic"))], m=5, maxit=50, method ='pmm', seed=500, printFlag = FALSE)
imp_df <-- complete(imp)
df_filtrado$orders_count_delta_1m <- imp_df$orders_count_delta_1m
df_filtrado$gmv_delta_1m <- imp_df$gmv_delta_1m
df_filtrado$distinct_skus_delta_1m <- imp_df$distinct_skus_delta_1m
df_filtrado$orders_count_delta_2m <- imp_df$orders_count_delta_2m
df_filtrado$gmv_delta_2m <- imp_df$gmv_delta_2m
df_filtrado$distinct_skus_delta_2m <- imp_df$distinct_skus_delta_2m
df_filtrado$mean_order_lapse <- imp_df$mean_order_lapse
# se imputa con ceros algunas colummnas que por errores sistemáticos están null
df_filtrado <- df_filtrado %>%
mutate(
warehouse_Bogota = coalesce(warehouse_Bogota, 0),
sub_categoria_Ambientadores = coalesce(sub_categoria_Ambientadores, 0),
sub_categoria_Complementos.y.vitaminas = coalesce(sub_categoria_Complementos.y.vitaminas, 0)
)
# Conteo de valores nulos para verificar
colSums(is.na(df_filtrado))
library(car)
modelo <- lm(is_churned ~ ., data = df_filtrado)
# Calcular VIF
vif_valores <- vif(modelo)
# Crear un dataframe para mostrar los nombres de las variables y sus VIFs
vif_df <- data.frame(VIF = vif_valores)
# Imprimir el dataframe
kable(vif_df, caption = "VIF values")
# Definir el umbral para el VIF
vif_threshold <- 5
# Crear una copia del dataframe original para no modificarlo directamente
df_reduction_copy <- df_filtrado
# Bucle para calcular VIF y eliminar la variable con el mayor VIF
while(TRUE) {
# Ajustar un modelo de regresión lineal usando 'is_churned' como la variable de respuesta
formula <- as.formula(paste("is_churned ~", paste(setdiff(names(df_reduction_copy), "is_churned"), collapse="+")))
modelo <- lm(formula, data=df_reduction_copy)
# Calcular VIF
vif_valores <- vif(modelo)
# Obtener el máximo VIF y el nombre de la variable correspondiente
max_vif <- max(vif_valores)
if (max_vif < vif_threshold) {
break
}
variable_max_vif <- names(vif_valores)[which.max(vif_valores)]
# Imprimir la variable que se está eliminando y su VIF
cat(sprintf("Dropping '%s' with VIF: %f\n", variable_max_vif, max_vif))
# Eliminar la variable con el VIF más alto
df_reduction_copy <- df_reduction_copy[, !(names(df_reduction_copy) %in% variable_max_vif)]
}
modelo <- lm(is_churned ~ ., data = df_reduction_copy)
# Calcular VIF
vif_valores <- vif(modelo)
# Crear un dataframe para mostrar los nombres de las variables y sus VIFs
vif_df <- data.frame(VIF = vif_valores)
# Imprimir el dataframe
kable(vif_df, caption = "VIF values")
df_final <- df_reduction_copy
library(DT)
datatable(tail(df_final, 10), options = list(scrollX = TRUE))
# Reemplazar 0 y 1 en la columna 'is_churned' con etiquetas
df_reduction_copy$is_churned <- factor(df_reduction_copy$is_churned, levels = c(0, 1), labels = c("not churned", "churned"))
# Crear el gráfico de conteo
ggplot(data = df_reduction_copy, aes(x = is_churned)) +
geom_bar(fill = "blue") +
labs(title = "Count of churned and non churned customers",
x = "",
y = "Count")
numerical_cols <- setdiff(names(df_reduction_copy), c('warehouse_Bogota', 'sub_categoria_Ambientadores', 'sub_categoria_Complementos.y.vitaminas', 'is_churned'))
# Crear un boxplot para cada columna numérica
for (col in numerical_cols) {
p <- ggplot(df_reduction_copy, aes_string(x = "is_churned", y = col)) +
geom_boxplot() +  # Elimina outliers
coord_flip() +  # Hace el boxplot horizontal
labs(title = paste("Boxplot of", col, "by is_churned"),
y = col,
x = "is_churned")
print(p)
}
df_categoricals_final <- df[, c(significant_columns, 'is_churned')]
df_categoricals_final$is_churned <- factor(df_categoricals_final$is_churned, levels = c(0, 1), labels = c("not churned", "churned"))
# Crear un gráfico de barras apiladas para cada variable categórica
for (col in significant_columns) {
# Calcular los porcentajes
df_percent <- df_categoricals_final %>%
group_by_at(col) %>%
count(is_churned) %>%
group_by_at(col) %>%
mutate(perc = n / sum(n) * 100)
# Crear el gráfico
p <- ggplot(df_percent, aes_string(x = col, y = "perc", fill = "is_churned")) +
geom_bar(stat = "identity", position = "fill") +
labs(title = paste("Porcentaje de is_churned por", col),
x = col,
y = "Porcentaje (%)") +
scale_y_continuous(labels = scales::percent)
print(p)
}
names(df_final)
library(caTools)
library(pROC)
df_final$warehouse_Bogota <- factor(df_final$warehouse_Bogota)
df_final$sub_categoria_Ambientadores <- factor(df_final$sub_categoria_Ambientadores)
df_final$sub_categoria_Complementos.y.vitaminas <- factor(df_final$sub_categoria_Complementos.y.vitaminas)
df_final$is_churned <- factor(df_final$is_churned)
set.seed(123)  # for reproducibility
split <- sample.split(df_final$is_churned, SplitRatio = 0.8)
train_set <- subset(df_final, split == TRUE)
test_set <- subset(df_final, split == FALSE)
# escalando los datos
# For train_set
numeric_columns <- sapply(train_set, is.numeric)
train_set[numeric_columns] <- scale(train_set[numeric_columns])
# For test_set
numeric_columns <- sapply(test_set, is.numeric)
test_set[numeric_columns] <- scale(test_set[numeric_columns])
write.csv(train_set, "train_set.csv", row.names = FALSE)
test <- read.csv("train_set.csv")
View(test)
str(train_set)
str(test)
test <- read.csv("train_set.csv")
test$warehouse_Bogota <- factor(test$warehouse_Bogota)
test$sub_categoria_Ambientadores <- factor(test$sub_categoria_Ambientadores)
test$sub_categoria_Complementos.y.vitaminas <- factor(test$sub_categoria_Complementos.y.vitaminas)
test$is_churned <- factor(test$is_churned)
str(test)
write.csv(test_set, "test_set.csv", row.names = FALSE)
test_set <- read.csv("test_set.csv")
train_set <- read.csv("train_set.csv")
test_set$warehouse_Bogota <- factor(test_set$warehouse_Bogota)
test_set$sub_categoria_Ambientadores <- factor(test_set$sub_categoria_Ambientadores)
test_set$sub_categoria_Complementos.y.vitaminas <- factor(test_set$sub_categoria_Complementos.y.vitaminas)
test_set$is_churned <- factor(test_set$is_churned)
train_set <- read.csv("train_set.csv")
train_set$warehouse_Bogota <- factor(train_set$warehouse_Bogota)
train_set$sub_categoria_Ambientadores <- factor(train_set$sub_categoria_Ambientadores)
train_set$sub_categoria_Complementos.y.vitaminas <- factor(train_set$sub_categoria_Complementos.y.vitaminas)
train_set$is_churned <- factor(train_set$is_churned)
str(train_set)
str(test_set)
runApp('shiny_app.R')
library(tidyverse)
df <- read.csv('churn_prediction_features.csv')
library(tidyverse)
df <- read.csv('churn_prediction_features.csv')
str(df)
categorical_cols <- names(df)[grepl("^sub_categoria_|^warehouse_", names(df))]
categorical_cols <- c(categorical_cols, 'customer_id', 'has_churned_before', 'is_churned')
numerical_cols <- setdiff(names(df), categorical_cols)
df_categoricals = subset(df, select = categorical_cols)
df_categoricals <- df_categoricals[, !(names(df_categoricals) %in% c("customer_id"))]
df_categoricals <- df_categoricals[-c(4168, 4713), ]
dim(df_categoricals)
# Inicializa un dataframe para almacenar los resultados
resultados <- data.frame(column_name = character(), negative_class = integer(), positive_class = integer())
# Calcula los conteos para cada columna
for (column_name in names(df_categoricals)) {
positive_count <- sum(df_categoricals[[column_name]] == 1)
negative_count <- sum(df_categoricals[[column_name]] == 0)
# Añade los resultados al dataframe
resultados <- rbind(resultados, data.frame(column_name, negative_class = negative_count, positive_class = positive_count))
}
library(knitr)
kable(head(resultados), caption = "Conteo de classes por variable")
resultados %>% filter(negative_class < 5)
resultados %>% filter(positive_class < 5)
df_categoricals <- df_categoricals[, !(names(df_categoricals) %in% c("sub_categoria_Detergentes.y.jabones"))]
library(dplyr)
p_values <- numeric(length = ncol(df_categoricals) - 1)
names(p_values) <- setdiff(colnames(df_categoricals), "warehouse_Bogota")
# Realizar las pruebas de chi-cuadrado
for (col in names(p_values)) {
# Crear la tabla de contingencia para cada columna con 'warehouse_Bogota'
contingency_table <- table(df_categoricals$warehouse_Bogota, df_categoricals[[col]])
# Realizar la prueba chi-cuadrado y almacenar el valor p
test_result <- chisq.test(contingency_table)
p_values[col] <- test_result$p.value
}
# Ahora, filtramos los nombres de las columnas con un valor p menor a 0.05
significant_columns <- names(p_values)[p_values > 0.05]
significant_columns <- significant_columns[!is.na(significant_columns)]
significant_columns <- c(significant_columns, "warehouse_Bogota")
significant_columns
df_independent_variables <- subset(df_categoricals, select = significant_columns)
p_values <- numeric(length = ncol(df_independent_variables) - 1)
names(p_values) <- setdiff(colnames(df_independent_variables), "sub_categoria_Ambientadores")
# Realizar las pruebas de chi-cuadrado
for (col in names(p_values)) {
# Crear la tabla de contingencia para cada columna con 'sub_categoria_Ambientadores'
contingency_table <- table(df_independent_variables$sub_categoria_Ambientadores, df_independent_variables[[col]])
# Realizar la prueba chi-cuadrado y almacenar el valor p
test_result <- chisq.test(contingency_table)
p_values[col] <- test_result$p.value
}
# Ahora, filtramos los nombres de las columnas con un valor p menor a 0.05
significant_columns <- names(p_values)[p_values > 0.05]
significant_columns <- significant_columns[!is.na(significant_columns)]
significant_columns <- c(significant_columns, "sub_categoria_Ambientadores")
significant_columns
df_independent_variables <- subset(df_independent_variables, select = significant_columns)
p_values <- numeric(length = ncol(df_independent_variables) - 1)
names(p_values) <- setdiff(colnames(df_independent_variables), "sub_categoria_Complementos.y.vitaminas")
# Realizar las pruebas de chi-cuadrado
for (col in names(p_values)) {
# Crear la tabla de contingencia para cada columna con 'sub_categoria_Complementos.y.vitaminas'
contingency_table <- table(df_independent_variables$sub_categoria_Complementos.y.vitaminas, df_independent_variables[[col]])
# Realizar la prueba chi-cuadrado y almacenar el valor p
test_result <- chisq.test(contingency_table)
p_values[col] <- test_result$p.value
}
# Ahora, filtramos los nombres de las columnas con un valor p menor a 0.05
significant_columns <- names(p_values)[p_values > 0.05]
significant_columns <- significant_columns[!is.na(significant_columns)]
significant_columns <- c(significant_columns, "sub_categoria_Complementos.y.vitaminas")
significant_columns
df_independent_variables <- subset(df_independent_variables, select = significant_columns)
library(reshape2)
cor_matrix <- cor(df[numerical_cols], use = "complete.obs")
# Transforma la matriz de correlación para la visualización
cor_melted <- melt(cor_matrix)
# Configura el tamaño de la fuente para los labels (ajusta según sea necesario)
font_size <- 6
ggplot(data = cor_melted, aes(x = Var1, y = Var2, fill = value)) +
geom_tile() +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1, 1), space = "Lab",
name="Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1, size = font_size),
axis.text.y = element_text(size = font_size)) +
coord_fixed() +
labs(x = '', y = '') +
geom_text(aes(label = sprintf("%.2f", value)), size = 3, check_overlap = TRUE)
columns_for_reduction = c(numerical_cols, significant_columns, 'is_churned')
df_reduction = subset(df, select = columns_for_reduction)
dim(df_reduction)
library(VIM)
aggr_plot <- aggr(df_reduction, col=c('navyblue', 'red'), numbers=FALSE, sortVars=TRUE,
cex.axis=0.4, gap=1, ylab=c("Histogram of missing data", "Pattern"))
porcentaje_nulos <- sapply(df_reduction, function(x) mean(is.na(x))) * 100
df_filtrado <- df_reduction[, porcentaje_nulos < 40]
names(df_filtrado)
library(mice)
porcentaje_nulos <- sapply(df_reduction, function(x) mean(is.na(x))) * 100
df_filtrado <- df_reduction[, porcentaje_nulos < 40]
imp <- mice(df_filtrado[, (names(df_filtrado) %in% c("orders_count_delta_1m", "gmv_delta_1m", "distinct_skus_delta_1m", "orders_count_delta_2m", "gmv_delta_2m", "distinct_skus_delta_2m", "mean_order_lapse", "antiquity_days",
"canceled_orders", "visits_count_3m", "gmv_3m", "AOV_3m", "distinct_sku_3m", "order_count_historic"))], m=5, maxit=50, method ='pmm', seed=500, printFlag = FALSE)
imp_df <-- complete(imp)
df_filtrado$orders_count_delta_1m <- imp_df$orders_count_delta_1m
df_filtrado$gmv_delta_1m <- imp_df$gmv_delta_1m
df_filtrado$distinct_skus_delta_1m <- imp_df$distinct_skus_delta_1m
df_filtrado$orders_count_delta_2m <- imp_df$orders_count_delta_2m
df_filtrado$gmv_delta_2m <- imp_df$gmv_delta_2m
df_filtrado$distinct_skus_delta_2m <- imp_df$distinct_skus_delta_2m
df_filtrado$mean_order_lapse <- imp_df$mean_order_lapse
# se imputa con ceros algunas colummnas que por errores sistemáticos están null
df_filtrado <- df_filtrado %>%
mutate(
warehouse_Bogota = coalesce(warehouse_Bogota, 0),
sub_categoria_Ambientadores = coalesce(sub_categoria_Ambientadores, 0),
sub_categoria_Complementos.y.vitaminas = coalesce(sub_categoria_Complementos.y.vitaminas, 0)
)
# Conteo de valores nulos para verificar
colSums(is.na(df_filtrado))
library(car)
modelo <- lm(is_churned ~ ., data = df_filtrado)
# Calcular VIF
vif_valores <- vif(modelo)
# Crear un dataframe para mostrar los nombres de las variables y sus VIFs
vif_df <- data.frame(VIF = vif_valores)
# Imprimir el dataframe
kable(vif_df, caption = "VIF values")
# Definir el umbral para el VIF
vif_threshold <- 5
# Crear una copia del dataframe original para no modificarlo directamente
df_reduction_copy <- df_filtrado
# Bucle para calcular VIF y eliminar la variable con el mayor VIF
while(TRUE) {
# Ajustar un modelo de regresión lineal usando 'is_churned' como la variable de respuesta
formula <- as.formula(paste("is_churned ~", paste(setdiff(names(df_reduction_copy), "is_churned"), collapse="+")))
modelo <- lm(formula, data=df_reduction_copy)
# Calcular VIF
vif_valores <- vif(modelo)
# Obtener el máximo VIF y el nombre de la variable correspondiente
max_vif <- max(vif_valores)
if (max_vif < vif_threshold) {
break
}
variable_max_vif <- names(vif_valores)[which.max(vif_valores)]
# Imprimir la variable que se está eliminando y su VIF
cat(sprintf("Dropping '%s' with VIF: %f\n", variable_max_vif, max_vif))
# Eliminar la variable con el VIF más alto
df_reduction_copy <- df_reduction_copy[, !(names(df_reduction_copy) %in% variable_max_vif)]
}
modelo <- lm(is_churned ~ ., data = df_reduction_copy)
# Calcular VIF
vif_valores <- vif(modelo)
# Crear un dataframe para mostrar los nombres de las variables y sus VIFs
vif_df <- data.frame(VIF = vif_valores)
# Imprimir el dataframe
kable(vif_df, caption = "VIF values")
df_final <- df_reduction_copy
library(DT)
datatable(tail(df_final, 10), options = list(scrollX = TRUE))
# Reemplazar 0 y 1 en la columna 'is_churned' con etiquetas
df_reduction_copy$is_churned <- factor(df_reduction_copy$is_churned, levels = c(0, 1), labels = c("not churned", "churned"))
# Crear el gráfico de conteo
ggplot(data = df_reduction_copy, aes(x = is_churned)) +
geom_bar(fill = "blue") +
labs(title = "Count of churned and non churned customers",
x = "",
y = "Count")
numerical_cols <- setdiff(names(df_reduction_copy), c('warehouse_Bogota', 'sub_categoria_Ambientadores', 'sub_categoria_Complementos.y.vitaminas', 'is_churned'))
# Crear un boxplot para cada columna numérica
for (col in numerical_cols) {
p <- ggplot(df_reduction_copy, aes_string(x = "is_churned", y = col)) +
geom_boxplot() +  # Elimina outliers
coord_flip() +  # Hace el boxplot horizontal
labs(title = paste("Boxplot of", col, "by is_churned"),
y = col,
x = "is_churned")
print(p)
}
df_categoricals_final <- df[, c(significant_columns, 'is_churned')]
df_categoricals_final$is_churned <- factor(df_categoricals_final$is_churned, levels = c(0, 1), labels = c("not churned", "churned"))
# Crear un gráfico de barras apiladas para cada variable categórica
for (col in significant_columns) {
# Calcular los porcentajes
df_percent <- df_categoricals_final %>%
group_by_at(col) %>%
count(is_churned) %>%
group_by_at(col) %>%
mutate(perc = n / sum(n) * 100)
# Crear el gráfico
p <- ggplot(df_percent, aes_string(x = col, y = "perc", fill = "is_churned")) +
geom_bar(stat = "identity", position = "fill") +
labs(title = paste("Porcentaje de is_churned por", col),
x = col,
y = "Porcentaje (%)") +
scale_y_continuous(labels = scales::percent)
print(p)
}
names(df_final)
library(caTools)
library(pROC)
df_final$warehouse_Bogota <- factor(df_final$warehouse_Bogota)
df_final$sub_categoria_Ambientadores <- factor(df_final$sub_categoria_Ambientadores)
df_final$sub_categoria_Complementos.y.vitaminas <- factor(df_final$sub_categoria_Complementos.y.vitaminas)
df_final$is_churned <- factor(df_final$is_churned)
set.seed(123)  # for reproducibility
split <- sample.split(df_final$is_churned, SplitRatio = 0.8)
train_set <- subset(df_final, split == TRUE)
test_set <- subset(df_final, split == FALSE)
# escalando los datos
# For train_set
numeric_columns <- sapply(train_set, is.numeric)
train_set[numeric_columns] <- scale(train_set[numeric_columns])
# For test_set
numeric_columns <- sapply(test_set, is.numeric)
test_set[numeric_columns] <- scale(test_set[numeric_columns])
write.csv(test_set, "test_set.csv", row.names = FALSE)
test <- read.csv("train_set.csv")
test$warehouse_Bogota <- factor(test$warehouse_Bogota)
test$sub_categoria_Ambientadores <- factor(test$sub_categoria_Ambientadores)
test$sub_categoria_Complementos.y.vitaminas <- factor(test$sub_categoria_Complementos.y.vitaminas)
test$is_churned <- factor(test$is_churned)
# Training a logistic regression model
model <- glm(is_churned ~ ., data = train_set, family = "binomial")
saveRDS(model, file = "logistic_regression.rds")
# Making predictions on the test set
predictions <- predict(model, newdata = test_set, type = "response")
predicted_class <- ifelse(predictions > 0.5, 1, 0)
# Creating a confusion matrix
confusion_matrix <- table(test_set$is_churned, predicted_class)
# Calculating precision, recall, and accuracy
precision <- confusion_matrix[2,2] / sum(confusion_matrix[,2])
recall <- confusion_matrix[2,2] / sum(confusion_matrix[2,])
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Calculating F1 score
f1_score <- 2 * (precision * recall) / (precision + recall)
# Printing metrics
print(paste("Precision:", precision))
print(paste("Recall:", recall))
print(paste("Accuracy:", accuracy))
print(paste("F1 Score:", f1_score))
# Calculating AUC
roc_curve <- roc(response = test_set$is_churned, predictor = predictions)
auc_value <- auc(roc_curve)
# Printing AUC
print(paste("AUC:", auc_value))
# Plotting the ROC curve
plot(roc_curve, main = "ROC Curve")
library(reshape2)
confusion_df <- as.data.frame(melt(confusion_matrix))
# Rename columns for clarity
colnames(confusion_df) <- c("Reference", "Prediction", "Count")
# Plot using ggplot2
ggplot(confusion_df, aes(x = Reference, y = Prediction, fill = Count)) +
geom_tile() +
geom_text(aes(label = Count), vjust = 1) +
scale_fill_gradient(low = "white", high = "blue") +
theme_minimal() +
labs(title = "Confusion Matrix", x = "Actual Class", y = "Predicted Class", fill = "Count")
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
install.packages("shinythemes")
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
runApp('shiny_app.R')
